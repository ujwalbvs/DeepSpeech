{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import math\n",
    "from math import ceil\n",
    "from sklearn import decomposition\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import json as JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_np_array(arr, filename):\n",
    "    assert(type(filename) == str)\n",
    "    np.save(filename, arr)\n",
    "\n",
    "def load_np_array(filename):\n",
    "    return np.load(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDINGS = 'output/embeddings/'\n",
    "LAYER4 = EMBEDDINGS + 'layer4/'\n",
    "LAYER5 = EMBEDDINGS + 'layer5/'\n",
    "LAYER6 = EMBEDDINGS + 'layer6/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "metadata": {},
   "outputs": [],
   "source": [
    "currgram = '1_gram/'\n",
    "TESTEMBEDDINGS = 'new_test_output/' + currgram + 'embeddings/'\n",
    "TESTLAYER4 = TESTEMBEDDINGS + 'layer4/'\n",
    "TESTLAYER5 = TESTEMBEDDINGS + 'layer5/'\n",
    "TESTLAYER6 = TESTEMBEDDINGS + 'layer6/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {},
   "outputs": [],
   "source": [
    "currtestlayer = TESTLAYER6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Time in ms\n",
    "Stride length in ms\n",
    "Sampling time in ms\n",
    "All lengths are relative to pool time, since pooling is a preprocessing step\n",
    "'''\n",
    "currlayer = LAYER6\n",
    "stride_time = 100\n",
    "pool_time = 100\n",
    "row_time = 500\n",
    "sampling_time = 20\n",
    "\n",
    "pool_length = pool_time//sampling_time\n",
    "feature_concats = row_time//pool_time\n",
    "stride_length = stride_time//pool_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_files_dict():\n",
    "    np_files = os.listdir(currlayer)\n",
    "    file_dict = {}\n",
    "    for file in np_files:\n",
    "        orig = file.split('_')[0]\n",
    "        if orig in file_dict:\n",
    "            file_dict[orig].append(file)\n",
    "        else:\n",
    "            file_dict[orig] = [file]\n",
    "    return file_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Row - Corresponds roughly to 1 unit time\n",
    "Bucket - Multiple Rows of features corresponding to 1 file\n",
    "Number of Rows in a bucket depends on the stride\n",
    "Columns - Same for all buckets, depends on the time\n",
    "'''\n",
    "\n",
    "def pool_one_batch(arr):\n",
    "    return np.max(arr, axis=0)\n",
    "\n",
    "def pool_full_matrix(arr):\n",
    "    rows = np.zeros(arr.shape[1])\n",
    "    for i in range(ceil(arr.shape[0]/pool_length)):\n",
    "        start_row = i*stride_length\n",
    "        end_row = (i+1)*stride_length\n",
    "        row = pool_one_batch(arr[start_row:end_row,:])\n",
    "        rows = np.vstack((rows,row))\n",
    "    return rows[1:,:]\n",
    "\n",
    "def concatenate_rows(arr, cols):\n",
    "    row = np.ones(0)\n",
    "    for i in range(arr.shape[0]):\n",
    "        row = np.hstack((row, arr[i,:]))\n",
    "    assert(len(row) <= cols)\n",
    "    row = np.hstack((row, np.zeros(cols-len(row))))\n",
    "    return row\n",
    "\n",
    "def extract_rows(arr):\n",
    "    cols = feature_concats*arr.shape[1]\n",
    "    rows = np.zeros((0,cols))\n",
    "    for i in range(ceil(arr.shape[0]/stride_length)):\n",
    "        start_row = i*stride_length\n",
    "        end_row = i*stride_length + feature_concats\n",
    "        row = concatenate_rows(arr[start_row:end_row,:], cols)\n",
    "        rows = np.vstack((rows, row))\n",
    "    return rows\n",
    "    \n",
    "def extract_bucket(np_arrs):\n",
    "    buck = np.zeros(feature_concats*np_arrs[0].shape[1])\n",
    "    for arr in np_arrs:\n",
    "        buck = np.vstack((buck, extract_rows(pool_full_matrix(arr))))\n",
    "    return buck[1:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_search_matrix():\n",
    "    file_dict = extract_files_dict()\n",
    "    buckets = np.zeros(0)\n",
    "    '''Start Index, End Index + 1, Filename'''\n",
    "    indexes = []\n",
    "    for key in file_dict:\n",
    "        np_arrs = []\n",
    "        for file in file_dict[key]:\n",
    "            np_arrs.append(load_np_array(currlayer + file))\n",
    "        bucket = extract_bucket(np_arrs)\n",
    "        if (buckets.shape[0] == 0):\n",
    "            start_index = 0\n",
    "            buckets = bucket\n",
    "        else:\n",
    "            start_index = buckets.shape[0]\n",
    "            buckets = np.vstack((buckets, bucket))\n",
    "        end_index = buckets.shape[0]\n",
    "        indexes.append((start_index, end_index, key))\n",
    "    return buckets, indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "metadata": {},
   "outputs": [],
   "source": [
    "buckets, indexes = create_search_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5051, 145)\n",
      "101\n"
     ]
    }
   ],
   "source": [
    "print(buckets.shape)\n",
    "print(len(indexes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNN:\n",
    "    def __init__(self, buckets, indexes, n_components=100):\n",
    "        self.buckets = buckets\n",
    "        self.indexes = indexes\n",
    "        self.v2 = self.precompute(self.buckets).reshape((-1,1))\n",
    "        self.buckmean = np.mean(self.buckets, axis=0)\n",
    "        self.pca = decomposition.PCA(n_components=n_components)\n",
    "        self.pcabuckets = self.buckets - self.buckmean\n",
    "        self.pcabuckets = self.pca.fit_transform(self.pcabuckets)\n",
    "        self.pcav2 = self.precompute(self.pcabuckets).reshape((-1,1))\n",
    "        \n",
    "    def precompute(self, buck):\n",
    "        arr = []\n",
    "        for i in range(buck.shape[0]):\n",
    "            arr.append(buck[i,:].dot(buck[i,:]))\n",
    "            ###Check this\n",
    "#             self.buckets[i,:] = self.buckets[i,:]/math.sqrt(arr[-1])\n",
    "#             arr[-1] = 1.0\n",
    "        return np.array(arr)\n",
    "    \n",
    "    def preprocess_inference(self, phrase_matrix, do_pca=False):\n",
    "        l2 = self.l2norms(phrase_matrix)\n",
    "        if do_pca:\n",
    "            return self.pca.transform(phrase_matrix - self.buckmean)\n",
    "        else:\n",
    "#             for i in range(phrase_matrix.shape[0]):\n",
    "#                 phrase_matrix[i,:] = phrase_matrix[i,:]/math.sqrt(l2[0][i])\n",
    "            return phrase_matrix\n",
    "        \n",
    "    def near_vector_scores(self, vector):\n",
    "        return (self.v2 + self.buckets.dot(vector.reshape((-1,1))) ) - vector.dot(vector)\n",
    "    \n",
    "    def l2norms(self, matrix):\n",
    "        return np.hstack([(np.ones(self.buckets.shape[0])*matrix[i,:].dot(matrix[i,:].T)).reshape((-1,1)) for i in range(matrix.shape[0])])\n",
    "    \n",
    "    def reduce_to_buckets(self, matrix_scores):\n",
    "        return np.sum(np.vstack([np.min(matrix_scores[ind[0]:ind[1],:], axis=0) for ind in self.indexes]), axis=1)\n",
    "    \n",
    "    '''\n",
    "    Here, phrase is a Matrix\n",
    "    Each Phrase is arranged in a row\n",
    "    '''\n",
    "    def near_bucket_scores(self, phrase_matrix):\n",
    "        matrix = self.preprocess_inference(phrase_matrix)\n",
    "        print(matrix.shape)\n",
    "        matrix_scores = self.v2 - 2*self.buckets.dot(matrix.T) + self.l2norms(matrix)\n",
    "#         matrix_scores = 2-2*self.buckets.dot(matrix.T)\n",
    "        print(matrix_scores.shape)\n",
    "        bucket_scores = self.reduce_to_buckets(matrix_scores)\n",
    "        return bucket_scores\n",
    "    \n",
    "    '''\n",
    "    Here, phrase is a Matrix\n",
    "    Each Phrase is arranged in a row\n",
    "    '''\n",
    "    def near_bucket_scores_pca(self, phrase_matrix):\n",
    "        matrix = self.preprocess_inference(phrase_matrix, do_pca=True)\n",
    "#         print(matrix.shape)\n",
    "        matrix_scores = self.pcav2 - 2*self.pcabuckets.dot(matrix.T) + self.l2norms(matrix)\n",
    "#         print(matrix_scores.shape)\n",
    "        bucket_scores = self.reduce_to_buckets(matrix_scores)\n",
    "        return bucket_scores\n",
    "    \n",
    "    def extract_nearby_buckets(self, phrase_matrix, threshold, do_pca=False):\n",
    "        arr = []\n",
    "        buck_scores = np.zeros(0)\n",
    "        if do_pca:\n",
    "            buck_scores = self.near_bucket_scores_pca(phrase_matrix)\n",
    "        else:\n",
    "            buck_scores = self.near_bucket_scores(phrase_matrix)\n",
    "        for i, score in enumerate(buck_scores):\n",
    "            if score < threshold:\n",
    "#                 print(score)\n",
    "                arr.append(i)\n",
    "        return arr\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNN(buckets, indexes, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(51, 145)\n",
      "(5051, 51)\n",
      "[22]\n"
     ]
    }
   ],
   "source": [
    "i = 22\n",
    "print(knn.extract_nearby_buckets(buckets[indexes[i][0]:indexes[i][1]-1,:], 0.00001, do_pca=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_test_files():\n",
    "#     print(currtestlayer)\n",
    "    np_files = os.listdir(currtestlayer)\n",
    "    file_dict = {}\n",
    "    for file in np_files:\n",
    "        orig = file.split('.')[0] + '.wav'\n",
    "#         if orig in file_dict:\n",
    "#             file_dict[orig].append(file)\n",
    "#         else:\n",
    "        file_dict[orig] = file\n",
    "    return file_dict\n",
    "\n",
    "with open('new_test_output/test_phrase_mapping_1.json') as f:\n",
    "    ground_truth_1_gram = JSON.load(f)\n",
    "with open('new_test_output/test_phrase_mapping_2.json') as f:\n",
    "    ground_truth_2_gram = JSON.load(f)\n",
    "with open('new_test_output/test_phrase_mapping_6.json') as f:\n",
    "    ground_truth_6_gram = JSON.load(f)\n",
    "    \n",
    "# print(ground_truth_1_gram)\n",
    "test_files_dict = extract_test_files()\n",
    "# print(test_files_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 145)\n",
      "(8, 145)\n",
      "(5051, 8)\n",
      "['000001426']\n",
      "[(355, 515, '000001355'), (1598, 1622, '000001381'), (1879, 1899, '000001390'), (2305, 2376, '000001399'), (2450, 2480, '000001401'), (2892, 2917, '000001405'), (3168, 3191, '000001411'), (3548, 3574, '000001419'), (4260, 4282, '000001437'), (4354, 4375, '000001440')]\n"
     ]
    }
   ],
   "source": [
    "# def test(file_dict):\n",
    "#     \n",
    "#\n",
    "test_buck = extract_bucket([load_np_array(currtestlayer + test_files_dict['TP_OD_1.wav'])])\n",
    "print(test_buck.shape)\n",
    "x = knn.extract_nearby_buckets(test_buck, 9000, do_pca=False)\n",
    "print(ground_truth_2_gram['TP_OD_1.wav'])\n",
    "print([indexes[y] for y in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 664,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 145)\n",
      "(1, 145)\n",
      "(5051, 1)\n",
      "[(355, 515, '000001355'), (1598, 1622, '000001381'), (1787, 1812, '000001387'), (1879, 1899, '000001390'), (2111, 2134, '000001396'), (2176, 2305, '000001398'), (2892, 2917, '000001405'), (3168, 3191, '000001411'), (3548, 3574, '000001419'), (3778, 3802, '000001426'), (3830, 3849, '000001428'), (4260, 4282, '000001437'), (4531, 4552, '000001445'), (4929, 5051, '000001451')]\n"
     ]
    }
   ],
   "source": [
    "test_buck = extract_bucket([load_np_array('sanity_test/embeddings/layer6/000001374_0.npy')[0:12]])\n",
    "print(test_buck.shape)\n",
    "# test_buck[10,3] = 1.0\n",
    "# test_buck[10,5] = 3.0\n",
    "# test_buck[10,7] = 2.0\n",
    "x = knn.extract_nearby_buckets(test_buck[1:2,:], 1000, do_pca=False)\n",
    "# print(ground_truth_2_gram['TP_OD_1.wav'])\n",
    "print([indexes[y] for y in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -6.430233  ,  -5.92282915,  -5.93382025,  -6.78656721,\n",
       "         -5.51487017,  -5.0272274 , -11.01980782,  -5.19922113,\n",
       "         -5.34897995,  -4.45374918, -10.27648354, -11.87998295,\n",
       "        -11.9861784 ,  -8.95866585,  -9.60204315,  -6.76203966,\n",
       "         -6.08948326, -18.74069214,  -9.28351688,  -5.58074236,\n",
       "         -3.26582623,  -8.85853004, -13.17168903,  -6.49048662,\n",
       "        -15.57391453,  -5.33720255, -17.69064713,  -6.66455173,\n",
       "         14.50683594,  -7.25215387,  -3.90565658,  -3.62913179,\n",
       "         -7.21197414,  -7.80587053,  -3.86678934,  -9.76060677,\n",
       "         -7.77058744,  -3.98853803,  -2.56414175,  -9.94606495,\n",
       "        -10.40016365, -11.06022549,  -9.71681595, -10.44680214,\n",
       "         -4.95135307,  -7.39020681, -18.38525772,  -9.37936878,\n",
       "         -6.58867645,  -3.78512263,  -5.93035316, -14.55838871,\n",
       "         -5.51263952, -18.73148155,  -2.24049592, -22.25370216,\n",
       "         -6.67302227,  15.77115154, -11.19170189,  -3.8334446 ,\n",
       "         -4.05012274,  -6.94281006, -10.0385046 ,  -2.93845153,\n",
       "         -9.64423943,  -8.47931767,  -3.7131772 ,  -2.57576346,\n",
       "        -10.92298222, -11.55590534, -10.30529499,  -8.38480759,\n",
       "        -10.51781845,  -5.79845142,  -7.17876816, -19.81799316,\n",
       "         -8.9261055 ,  -6.17455864,  -4.8384881 ,  -5.06249189,\n",
       "        -14.10869217,  -4.77727652, -19.83436012,  -2.43255258,\n",
       "        -23.3766613 ,  -8.92978287,  14.83960247, -11.92374802,\n",
       "         -0.55512166,  -6.69332838,  -9.78009033, -13.1542778 ,\n",
       "         -0.97081137, -13.42537403,  -9.46552753,  -6.07762337,\n",
       "         -0.23728648, -12.8857584 , -14.61214924, -11.56368923,\n",
       "         -9.76452351,  -9.4257431 ,  -2.94683814,  -9.04130936,\n",
       "        -21.01361275, -10.11329174,  -7.99370337,  -7.53587866,\n",
       "         -2.73887491, -16.3804493 ,  -7.35948753, -23.45333481,\n",
       "         -3.18660188, -26.28725815,  -7.48314619,  15.30130291,\n",
       "        -12.15908432,  -0.51578903,  -7.17756748,  -8.90847111,\n",
       "        -12.22230339,  -0.89040041,  -8.78688717,  -9.43107605,\n",
       "         -7.4661622 ,  -1.13508356, -14.69628048, -13.759058  ,\n",
       "         -9.91459942,  -5.77322388,  -5.54074907,  -4.81846189,\n",
       "         -7.82071447, -20.93606186,  -7.80487823,  -5.97457552,\n",
       "         -5.64611816,  -2.1498692 , -12.7296133 ,  -7.62635994,\n",
       "        -19.07443237,  -3.84006858, -23.77415657,  -7.05395222,\n",
       "         14.69602108]])"
      ]
     },
     "execution_count": 623,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_buck[:1,:] #- buckets[:1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1311, 1342, '000001374')\n",
      "(1086, 1139, '000001369')\n",
      "(1879, 1899, '000001390')\n",
      "(2111, 2134, '000001396')\n",
      "(2892, 2917, '000001405')\n",
      "(3168, 3191, '000001411')\n"
     ]
    }
   ],
   "source": [
    "# print(indexes[x])\n",
    "print(indexes[23])\n",
    "print(indexes[18])\n",
    "print(indexes[39])\n",
    "print(indexes[45])\n",
    "print(indexes[54])\n",
    "print(indexes[60])\n",
    "# indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    print(indexes[i][0])\n",
    "    print(indexes[i][1])\n",
    "    '''Sanity Check'''\n",
    "    scores = knn.near_bucket_scores_pca(buckets[indexes[i][0]:indexes[i][1],:])\n",
    "    # scores = knn.near_bucket_scores(np.vstack((np.zeros(290), 20*np.ones(290), 10*np.ones(290))))\n",
    "    \n",
    "    assert(np.argmin(scores) == i)\n",
    "    (indexes[np.argmin(scores)], np.min(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 3.]\n",
      " [4. 1. 2.]\n",
      " [5. 1. 6.]\n",
      " [3. 1. 1.]\n",
      " [1. 1. 8.]\n",
      " [1. 1. 3.]\n",
      " [4. 1. 2.]\n",
      " [5. 1. 6.]\n",
      " [3. 1. 1.]\n",
      " [1. 1. 8.]\n",
      " [1. 1. 3.]\n",
      " [4. 1. 2.]\n",
      " [5. 1. 6.]\n",
      " [3. 1. 1.]\n",
      " [1. 1. 8.]]\n",
      "[[3.33333333 1.         3.66666667]\n",
      " [1.66666667 1.         4.        ]\n",
      " [4.         1.         3.        ]\n",
      " [2.         1.         4.33333333]\n",
      " [3.         1.         5.        ]]\n",
      "[[3.33333333 1.         3.66666667 1.66666667 1.         4.\n",
      "  4.         1.         3.        ]\n",
      " [2.         1.         4.33333333 3.         1.         5.\n",
      "  0.         0.         0.        ]]\n",
      "[[3.33333333 1.         3.66666667 1.66666667 1.         4.\n",
      "  4.         1.         3.        ]\n",
      " [2.         1.         4.33333333 3.         1.         5.\n",
      "  0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "n = np.ones((5,3))\n",
    "n[0][0] = 1\n",
    "n[0][2] = 3\n",
    "n[1][2] = 2\n",
    "n[1][0] = 4\n",
    "n[2][0] = 5\n",
    "n[2][2] = 6\n",
    "n[3][0] = 3\n",
    "n[4][2] = 8\n",
    "n = np.vstack((n,n,n))\n",
    "print(n)\n",
    "feature_concats = 3\n",
    "stride_length = 3\n",
    "print(extract_bucket([n]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0"
      ]
     },
     "execution_count": 570,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n[4,:] + n[4,:].dot(n[4,:].T)\n",
    "n[0,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 3.]\n",
      " [4. 1. 2.]\n",
      " [5. 1. 6.]]\n",
      "[[3.]\n",
      " [2.]\n",
      " [6.]]\n",
      "[[14. 24. 65.]\n",
      " [13. 23. 64.]\n",
      " [17. 27. 68.]]\n"
     ]
    }
   ],
   "source": [
    "mat = n[:3,:]\n",
    "print(mat)\n",
    "print(mat[:,2].reshape((-1,1)))\n",
    "print(np.hstack([(np.ones(mat.shape[1])*mat[i,:].dot(mat[i,:].T)).reshape((-1,1)) for i in range(mat.shape[0])]) + mat[:,2].reshape((-1,1))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mappings = json.load('')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
